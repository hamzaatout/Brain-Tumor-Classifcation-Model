{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "In this notebook, we aim to predict **Formula 1 race positions** using **historical data**. By merging multiple datasets on races, drivers, constructors, and circuits, we will **prepare, train,** and **evaluate machine learning models** to uncover patterns in race outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "restructuredtext"
    }
   },
   "source": [
    "#### **Import the following Libraries:**\n",
    "1. os: for interacting with the operating system\n",
    "1. Numpy: for numerical computing\n",
    "2. Pandas: for data manipulation & analysis\n",
    "3. Matplotlib: for data visualization\n",
    "4. Tensorflow: for large scale machine learning tasks\n",
    "5. Keras: for simplifying building & traing the neural network\n",
    "6. sklearn: used for machine-learning tasks\n",
    "7. PIL: for image processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For neural networks\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import xgboost as xgb\n",
    "\n",
    "# For data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In this section, we load the essential datasets required for our analysis using the pandas library. The datasets contain historical Formula 1 data, which we'll use to predict race positions._\n",
    "\n",
    "#### **Datasets Loaded:**\n",
    "1. **drivers.csv:** Contains information about drivers.\n",
    "2. **constructors.csv:** Contains information about constructors (teams).\n",
    "3. **races.csv:** Details of each race event.\n",
    "4. **results.csv:** Race results for each driver in each race.\n",
    "5. **qualifying.csv:** Qualifying results for each driver.\n",
    "6. **circuits.csv:** Information about the circuits where races are held."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "drivers = pd.read_csv('drivers.csv')\n",
    "constructors = pd.read_csv('constructors.csv')\n",
    "races = pd.read_csv('races.csv')\n",
    "results = pd.read_csv('results.csv')\n",
    "qualifying = pd.read_csv('qualifying.csv')\n",
    "circuits = pd.read_csv('circuits.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In this section, we merge the various datasets to create a comprehensive dataset for analysis and perform initial data cleaning steps._\n",
    "\n",
    "#### **Data Merging:**\n",
    "\n",
    "1. **Merge results with races:** Adds race details like year, round, and circuitId to the results dataframe.\n",
    "2. **Merge with drivers:** Incorporates driver details such as driverRef (a reference identifier) and nationality.\n",
    "3. **Merge with constructors:** Adds constructor (team) names to the dataset.\n",
    "4. **Merge with circuits:** Includes circuit information like name, location, and country. We use suffixes to distinguish between columns with the same name from different tables.\n",
    "\n",
    "#### **Data Cleaning:**\n",
    "\n",
    "**Preview Merged Dataset:** Use results.head() to display the first few rows and verify the merges.\n",
    "\n",
    "**Check for Missing Values:** Identify any missing data that might affect analysis using results.isnull().sum().\n",
    "\n",
    "**Handle Missing position Values:**\n",
    "1. Remove rows where 'position' is missing since it's our target variable.\n",
    "2. Convert the 'position' column to numeric, coercing non-numeric values (like '\\\\N') to NaN.\n",
    "3. Drop rows with NaN in 'position'.\n",
    "4. Convert the cleaned 'position' column to integer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge results with races\n",
    "results = results.merge(races[['raceId', 'year', 'round', 'circuitId']], on='raceId', how='left')\n",
    "\n",
    "# Merge with drivers\n",
    "results = results.merge(drivers[['driverId', 'driverRef', 'nationality']], on='driverId', how='left')\n",
    "\n",
    "# Merge with constructors\n",
    "results = results.merge(constructors[['constructorId', 'name']], on='constructorId', how='left')\n",
    "\n",
    "# Merge with circuits\n",
    "results = results.merge(circuits[['circuitId', 'name', 'location', 'country']], on='circuitId', how='left', suffixes=('_constructor', '_circuit'))\n",
    "\n",
    "# Preview the merged dataset\n",
    "results.head()\n",
    "\n",
    "# Check for missing values\n",
    "results.isnull().sum()\n",
    "\n",
    "# Since we're predicting 'position', remove rows with missing positions\n",
    "results = results[results['position'].notna()]\n",
    "\n",
    "# Convert 'position' to numeric\n",
    "# Replace '\\\\N' and other non-numeric values with NaN\n",
    "results['position'] = pd.to_numeric(results['position'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN in 'position'\n",
    "results = results.dropna(subset=['position'])\n",
    "\n",
    "# Convert 'position' to integer\n",
    "results['position'] = results['position'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Here, we prepare the data for modeling by selecting relevant features, encoding categorical variables, and scaling the features._\n",
    "\n",
    "#### **Feature Selection:**\n",
    "\n",
    "**Selected features that may influence race outcomes:**\n",
    "1. 'year', 'round': Temporal features of the race.\n",
    "2. 'driverRef', 'nationality': Driver-specific information.\n",
    "3. 'name_constructor': Team information, renamed to 'constructorName'.\n",
    "4. 'grid': The starting position on the grid.\n",
    "5. 'positionOrder': The finishing position, renamed to 'position' (our target variable).\n",
    "6. 'points', 'laps': Performance metrics.\n",
    "7. 'statusId': Indicates the race status of the driver.\n",
    "\n",
    "**Important Note on Data Leakage:**\n",
    "\n",
    "_Caution:_ Including 'positionOrder' and possibly 'points' and 'laps' as features may lead to data leakage since they are outcomes of the race, not inputs known beforehand. This could artificially inflate model performance.\n",
    "\n",
    "#### **Data Preparation:**\n",
    "\n",
    "**Create New DataFrame:** data contains only the selected features.\n",
    "\n",
    "**Rename Columns:** For clarity, 'name_constructor' to 'constructorName' and 'positionOrder' to 'position'.\n",
    "\n",
    "**Encode Categorical Variables:**\n",
    "Use LabelEncoder to transform 'driverRef', 'nationality', and 'constructorName' into numeric values.\n",
    "Define Features and Target Variable:\n",
    "X: Feature matrix (all columns except 'position').\n",
    "y: Target vector ('position').\n",
    "\n",
    "**Feature Scaling:**\n",
    "Apply MinMaxScaler to scale features between 0 and 1, which helps improve the model's convergence during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "features = ['year', 'round', 'driverRef', 'nationality', 'name_constructor', 'grid', 'positionOrder', 'points', 'laps', 'statusId']\n",
    "\n",
    "# Create a new dataframe with selected features\n",
    "data = results[features]\n",
    "\n",
    "# Rename columns for clarity\n",
    "data.rename(columns={'name_constructor': 'constructorName', 'positionOrder': 'position'}, inplace=True)\n",
    "\n",
    "# Encode 'driverRef', 'nationality', and 'constructorName'\n",
    "label_encoders = {}\n",
    "\n",
    "for column in ['driverRef', 'nationality', 'constructorName']:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(['position'], axis=1)\n",
    "y = data['position']\n",
    "\n",
    "# Use MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Splitting our data into training & testing sets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We define our initial neural network model using Keras' Sequential API._\n",
    "\n",
    "#### Model Architecture:\n",
    "\n",
    "**Input Layer:** Specifies the input shape based on the number of features.\n",
    "\n",
    "**First Hidden Layer:**\n",
    "\n",
    "Dense layer with 64 neurons and ReLU activation.\n",
    "Dropout layer with a rate of 0.2 to prevent overfitting.\n",
    "\n",
    "**Second Hidden Layer:**\n",
    "\n",
    "Dense layer with 32 neurons and ReLU activation.\n",
    "\n",
    "**Output Layer:**\n",
    "\n",
    "Dense layer with 1 neuron (since it's a regression problem predicting a continuous value).\n",
    "\n",
    "_This model serves as a baseline to compare with more complex architectures._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We compile, train, and evaluate the initial neural network model._\n",
    "\n",
    "#### **Compile the Model:**\n",
    "\n",
    "**Loss Function:** 'mean_squared_error' suitable for regression tasks.\n",
    "\n",
    "**Optimizer:** 'adam' optimizer for efficient gradient descent.\n",
    "\n",
    "#### **Train the Model:**\n",
    "\n",
    "Train for 50 epochs with a batch size of 32.\n",
    "Use a 10% validation split to monitor the model's performance on unseen data during training.\n",
    "\n",
    "#### **Predict and Evaluate:**\n",
    "\n",
    "Make predictions on the test set.\n",
    "\n",
    "#### **Calculate evaluation metrics:**\n",
    "\n",
    "**Mean Squared Error (MSE):** Measures the average squared difference between predicted and actual values.\n",
    "\n",
    "**R² Score:** Indicates the proportion of variance in the dependent variable predictable from the independent variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 37.9227 - val_loss: 8.3339\n",
      "Epoch 2/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.2710 - val_loss: 6.4967\n",
      "Epoch 3/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.9290 - val_loss: 5.8425\n",
      "Epoch 4/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.0311 - val_loss: 5.1150\n",
      "Epoch 5/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5440 - val_loss: 4.6386\n",
      "Epoch 6/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.3392 - val_loss: 4.4798\n",
      "Epoch 7/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2303 - val_loss: 4.4246\n",
      "Epoch 8/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.9462 - val_loss: 4.1001\n",
      "Epoch 9/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.8410 - val_loss: 4.0222\n",
      "Epoch 10/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6666 - val_loss: 3.9395\n",
      "Epoch 11/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6260 - val_loss: 3.8709\n",
      "Epoch 12/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.5586 - val_loss: 3.7803\n",
      "Epoch 13/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.5637 - val_loss: 3.7513\n",
      "Epoch 14/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2823 - val_loss: 3.7256\n",
      "Epoch 15/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.3109 - val_loss: 3.7089\n",
      "Epoch 16/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1660 - val_loss: 3.8298\n",
      "Epoch 17/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2764 - val_loss: 3.8094\n",
      "Epoch 18/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3775 - val_loss: 3.5224\n",
      "Epoch 19/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.0457 - val_loss: 3.6182\n",
      "Epoch 20/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0960 - val_loss: 3.4973\n",
      "Epoch 21/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2378 - val_loss: 3.5806\n",
      "Epoch 22/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0211 - val_loss: 3.5129\n",
      "Epoch 23/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0019 - val_loss: 3.4649\n",
      "Epoch 24/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9180 - val_loss: 3.5323\n",
      "Epoch 25/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9400 - val_loss: 3.3797\n",
      "Epoch 26/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7565 - val_loss: 3.3471\n",
      "Epoch 27/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9526 - val_loss: 3.4087\n",
      "Epoch 28/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8041 - val_loss: 3.3230\n",
      "Epoch 29/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8230 - val_loss: 3.3066\n",
      "Epoch 30/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9084 - val_loss: 3.2976\n",
      "Epoch 31/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8860 - val_loss: 3.3251\n",
      "Epoch 32/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7364 - val_loss: 3.3278\n",
      "Epoch 33/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9220 - val_loss: 3.5544\n",
      "Epoch 34/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7835 - val_loss: 3.2376\n",
      "Epoch 35/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6214 - val_loss: 3.3493\n",
      "Epoch 36/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6246 - val_loss: 3.2262\n",
      "Epoch 37/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7563 - val_loss: 3.2284\n",
      "Epoch 38/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5792 - val_loss: 3.2165\n",
      "Epoch 39/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5653 - val_loss: 3.2093\n",
      "Epoch 40/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5793 - val_loss: 3.2259\n",
      "Epoch 41/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7212 - val_loss: 3.4425\n",
      "Epoch 42/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7729 - val_loss: 3.1482\n",
      "Epoch 43/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6008 - val_loss: 3.2395\n",
      "Epoch 44/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - loss: 3.4404 - val_loss: 3.2119\n",
      "Epoch 45/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5372 - val_loss: 3.1675\n",
      "Epoch 46/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step - loss: 3.4524 - val_loss: 3.2024\n",
      "Epoch 47/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5256 - val_loss: 3.1881\n",
      "Epoch 48/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4339 - val_loss: 3.1221\n",
      "Epoch 49/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5670 - val_loss: 3.2126\n",
      "Epoch 50/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4999 - val_loss: 3.1683\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Mean Squared Error: 3.4523397493463035\n",
      "R^2 Score: 0.852258255599734\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We plot the training and validation loss over epochs to assess the model's learning behavior._\n",
    "\n",
    "#### Plot Loss Curves:\n",
    "Visualize how the loss decreases over time for both training and validation sets.\n",
    "Helps identify issues like overfitting (validation loss starts increasing while training loss continues to decrease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We experiment with a deeper neural network to see if increasing the model's complexity improves performance._\n",
    "\n",
    "#### Model Architecture:\n",
    "\n",
    "**First Hidden Layer:**\n",
    "Dense layer with 128 neurons and ReLU activation.\n",
    "Dropout layer with a rate of 0.3.\n",
    "\n",
    "**Second Hidden Layer:**\n",
    "Dense layer with 64 neurons and ReLU activation.\n",
    "Dropout layer with a rate of 0.3.\n",
    "\n",
    "**Third Hidden Layer:**\n",
    "Dense layer with 32 neurons and ReLU activation.\n",
    "\n",
    "**Output Layer:**\n",
    "Dense layer with 1 neuron for regression output.\n",
    "\n",
    "_Compile, Train, and Evaluate:Use the same loss function and optimizer. Train the model and evaluate its performance using MSE and R² Score._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 33.0261 - val_loss: 6.9709\n",
      "Epoch 2/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.0332 - val_loss: 5.8201\n",
      "Epoch 3/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.7263 - val_loss: 5.0239\n",
      "Epoch 4/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 6.0801 - val_loss: 4.3851\n",
      "Epoch 5/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.9082 - val_loss: 4.6698\n",
      "Epoch 6/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.7489 - val_loss: 4.7785\n",
      "Epoch 7/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.1388 - val_loss: 3.9926\n",
      "Epoch 8/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 5.0081 - val_loss: 4.1947\n",
      "Epoch 9/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0419 - val_loss: 4.0067\n",
      "Epoch 10/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8456 - val_loss: 3.7374\n",
      "Epoch 11/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.8228 - val_loss: 3.6533\n",
      "Epoch 12/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4900 - val_loss: 3.7873\n",
      "Epoch 13/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3307 - val_loss: 3.6768\n",
      "Epoch 14/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.4636 - val_loss: 3.9428\n",
      "Epoch 15/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3572 - val_loss: 3.5895\n",
      "Epoch 16/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1554 - val_loss: 3.5088\n",
      "Epoch 17/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.3646 - val_loss: 3.5441\n",
      "Epoch 18/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0461 - val_loss: 3.3423\n",
      "Epoch 19/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.9417 - val_loss: 3.2520\n",
      "Epoch 20/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3.9726 - val_loss: 3.3854\n",
      "Epoch 21/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.0622 - val_loss: 3.5566\n",
      "Epoch 22/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3.9727 - val_loss: 3.3532\n",
      "Epoch 23/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8730 - val_loss: 3.1977\n",
      "Epoch 24/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8137 - val_loss: 3.2865\n",
      "Epoch 25/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9510 - val_loss: 3.3132\n",
      "Epoch 26/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8034 - val_loss: 3.2638\n",
      "Epoch 27/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7980 - val_loss: 3.1824\n",
      "Epoch 28/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6890 - val_loss: 3.2290\n",
      "Epoch 29/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9004 - val_loss: 3.2178\n",
      "Epoch 30/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5835 - val_loss: 3.4148\n",
      "Epoch 31/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6403 - val_loss: 3.2002\n",
      "Epoch 32/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7163 - val_loss: 3.2360\n",
      "Epoch 33/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7071 - val_loss: 3.1908\n",
      "Epoch 34/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5900 - val_loss: 3.1960\n",
      "Epoch 35/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7371 - val_loss: 3.2043\n",
      "Epoch 36/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5981 - val_loss: 3.2773\n",
      "Epoch 37/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5643 - val_loss: 3.2488\n",
      "Epoch 38/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7951 - val_loss: 3.0790\n",
      "Epoch 39/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947us/step - loss: 3.7358 - val_loss: 3.1630\n",
      "Epoch 40/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 3.4577 - val_loss: 3.1411\n",
      "Epoch 41/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5534 - val_loss: 3.2132\n",
      "Epoch 42/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6201 - val_loss: 3.0610\n",
      "Epoch 43/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step - loss: 3.5076 - val_loss: 3.1487\n",
      "Epoch 44/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7111 - val_loss: 3.1237\n",
      "Epoch 45/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4259 - val_loss: 3.3726\n",
      "Epoch 46/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4077 - val_loss: 3.2469\n",
      "Epoch 47/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step - loss: 3.5403 - val_loss: 3.1371\n",
      "Epoch 48/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4519 - val_loss: 3.1427\n",
      "Epoch 49/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3356 - val_loss: 3.0839\n",
      "Epoch 50/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.3029 - val_loss: 3.0996\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995us/step\n",
      "Mean Squared Error: 3.3214900299543983\n",
      "R^2 Score: 0.8578579263160665\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model2.add(Dropout(0.3))\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dropout(0.3))\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "model2.add(Dense(1))  # Output layer for regression\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "history = model2.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model2.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Here, we test a model with more neurons per layer to increase its capacity._\n",
    "\n",
    "#### Model Architecture:\n",
    "\n",
    "**First Hidden Layer:**\n",
    "Dense layer with 256 neurons and ReLU activation.\n",
    "Dropout layer with a rate of 0.4.\n",
    "\n",
    "**Second Hidden Layer:**\n",
    "Dense layer with 256 neurons and ReLU activation.\n",
    "Dropout layer with a rate of 0.4.\n",
    "\n",
    "**Output Layer:**\n",
    "Dense layer with 1 neuron for regression output.\n",
    "Compile, Train, and Evaluate:\n",
    "Same compilation and evaluation process as before.\n",
    "\n",
    "#### Purpose:\n",
    "To determine if a wider network captures more complex patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 27.1847 - val_loss: 6.4636\n",
      "Epoch 2/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.0470 - val_loss: 5.1167\n",
      "Epoch 3/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 6.2149 - val_loss: 4.6301\n",
      "Epoch 4/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.8450 - val_loss: 4.2314\n",
      "Epoch 5/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 5.2806 - val_loss: 4.2616\n",
      "Epoch 6/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.2281 - val_loss: 3.9287\n",
      "Epoch 7/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.1175 - val_loss: 3.8183\n",
      "Epoch 8/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.7787 - val_loss: 3.8580\n",
      "Epoch 9/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.9273 - val_loss: 3.6755\n",
      "Epoch 10/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.9713 - val_loss: 3.8045\n",
      "Epoch 11/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.6582 - val_loss: 3.5780\n",
      "Epoch 12/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.7985 - val_loss: 3.6349\n",
      "Epoch 13/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.3922 - val_loss: 3.5391\n",
      "Epoch 14/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.5189 - val_loss: 3.4955\n",
      "Epoch 15/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.4013 - val_loss: 3.3953\n",
      "Epoch 16/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.3278 - val_loss: 3.6660\n",
      "Epoch 17/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2637 - val_loss: 3.5655\n",
      "Epoch 18/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.5439 - val_loss: 3.4456\n",
      "Epoch 19/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.4581 - val_loss: 3.3362\n",
      "Epoch 20/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.2363 - val_loss: 3.3537\n",
      "Epoch 21/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.1733 - val_loss: 3.3374\n",
      "Epoch 22/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.3632 - val_loss: 3.7078\n",
      "Epoch 23/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.2434 - val_loss: 3.3669\n",
      "Epoch 24/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.1717 - val_loss: 3.7988\n",
      "Epoch 25/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.1236 - val_loss: 3.2899\n",
      "Epoch 26/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.0720 - val_loss: 3.3067\n",
      "Epoch 27/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.1087 - val_loss: 3.2322\n",
      "Epoch 28/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.2557 - val_loss: 3.2188\n",
      "Epoch 29/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.0794 - val_loss: 3.2091\n",
      "Epoch 30/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3.9649 - val_loss: 3.2984\n",
      "Epoch 31/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.1433 - val_loss: 3.2661\n",
      "Epoch 32/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3.9834 - val_loss: 3.1606\n",
      "Epoch 33/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.9704 - val_loss: 3.3364\n",
      "Epoch 34/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.9944 - val_loss: 3.1946\n",
      "Epoch 35/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.9574 - val_loss: 3.1646\n",
      "Epoch 36/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.8889 - val_loss: 3.2808\n",
      "Epoch 37/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.0740 - val_loss: 3.2280\n",
      "Epoch 38/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3.9141 - val_loss: 3.1601\n",
      "Epoch 39/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9117 - val_loss: 3.0793\n",
      "Epoch 40/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8157 - val_loss: 3.1691\n",
      "Epoch 41/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9415 - val_loss: 3.2061\n",
      "Epoch 42/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9537 - val_loss: 3.1711\n",
      "Epoch 43/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3.9491 - val_loss: 3.0386\n",
      "Epoch 44/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3.9176 - val_loss: 3.1662\n",
      "Epoch 45/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8585 - val_loss: 3.1682\n",
      "Epoch 46/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3.6807 - val_loss: 3.1847\n",
      "Epoch 47/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8157 - val_loss: 3.3504\n",
      "Epoch 48/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7057 - val_loss: 3.2300\n",
      "Epoch 49/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7207 - val_loss: 3.1325\n",
      "Epoch 50/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3.8624 - val_loss: 3.0944\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step\n",
      "Mean Squared Error: 3.286009651681681\n",
      "R^2 Score: 0.85937629743785\n"
     ]
    }
   ],
   "source": [
    "# Wider Neural Network\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(256, input_dim=X_train.shape[1], activation='relu'))\n",
    "model3.add(Dropout(0.4))\n",
    "model3.add(Dense(256, activation='relu'))\n",
    "model3.add(Dropout(0.4))\n",
    "model3.add(Dense(1))  # Output layer for regression\n",
    "\n",
    "# Compile the model\n",
    "model3.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "history = model3.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model3.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We modify the activation functions to see if it enhances model performance._\n",
    "\n",
    "#### Model Architecture:\n",
    "\n",
    "**First Hidden Layer:**\n",
    "Dense layer with 64 neurons.\n",
    "LeakyReLU activation function (alpha=0.1) to address the \"dying ReLU\" problem.\n",
    "Dropout layer with a rate of 0.2.\n",
    "\n",
    "**Second Hidden Layer:**\n",
    "Dense layer with 32 neurons.\n",
    "LeakyReLU activation function.\n",
    "\n",
    "**Output Layer:**\n",
    "Dense layer with 1 neuron for regression output.\n",
    "Compile, Train, and Evaluate:\n",
    "Use the same process as previous models.\n",
    "\n",
    "#### Purpose:\n",
    "To test whether alternative activation functions can improve learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 37.7555 - val_loss: 8.7011\n",
      "Epoch 2/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.4462 - val_loss: 6.7800\n",
      "Epoch 3/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.2751 - val_loss: 6.2092\n",
      "Epoch 4/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969us/step - loss: 6.6182 - val_loss: 5.7736\n",
      "Epoch 5/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.4060 - val_loss: 5.4712\n",
      "Epoch 6/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 982us/step - loss: 6.0493 - val_loss: 5.1712\n",
      "Epoch 7/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 5.5348 - val_loss: 5.0648\n",
      "Epoch 8/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 5.8914 - val_loss: 4.7724\n",
      "Epoch 9/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 5.3660 - val_loss: 4.5738\n",
      "Epoch 10/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.2072 - val_loss: 4.4757\n",
      "Epoch 11/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - loss: 4.8799 - val_loss: 4.3263\n",
      "Epoch 12/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.8517 - val_loss: 4.1598\n",
      "Epoch 13/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 4.7585 - val_loss: 4.0471\n",
      "Epoch 14/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9531 - val_loss: 3.9956\n",
      "Epoch 15/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.5785 - val_loss: 3.9370\n",
      "Epoch 16/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 4.6459 - val_loss: 3.8521\n",
      "Epoch 17/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step - loss: 4.3288 - val_loss: 3.9103\n",
      "Epoch 18/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2372 - val_loss: 3.7882\n",
      "Epoch 19/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 4.3221 - val_loss: 3.8181\n",
      "Epoch 20/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2390 - val_loss: 3.7170\n",
      "Epoch 21/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2778 - val_loss: 3.8112\n",
      "Epoch 22/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3205 - val_loss: 3.7610\n",
      "Epoch 23/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1270 - val_loss: 3.5989\n",
      "Epoch 24/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.0940 - val_loss: 3.6390\n",
      "Epoch 25/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1347 - val_loss: 3.5965\n",
      "Epoch 26/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0622 - val_loss: 3.6365\n",
      "Epoch 27/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8855 - val_loss: 3.6160\n",
      "Epoch 28/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9060 - val_loss: 3.6441\n",
      "Epoch 29/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9164 - val_loss: 3.5757\n",
      "Epoch 30/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9549 - val_loss: 3.4957\n",
      "Epoch 31/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9727 - val_loss: 3.4976\n",
      "Epoch 32/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7953 - val_loss: 3.5001\n",
      "Epoch 33/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0922 - val_loss: 3.5144\n",
      "Epoch 34/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9423 - val_loss: 3.5081\n",
      "Epoch 35/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 3.8985 - val_loss: 3.4723\n",
      "Epoch 36/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8112 - val_loss: 3.3938\n",
      "Epoch 37/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9879 - val_loss: 3.3682\n",
      "Epoch 38/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6839 - val_loss: 3.4400\n",
      "Epoch 39/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7533 - val_loss: 3.3514\n",
      "Epoch 40/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.8781 - val_loss: 3.3756\n",
      "Epoch 41/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7269 - val_loss: 3.3299\n",
      "Epoch 42/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7695 - val_loss: 3.3459\n",
      "Epoch 43/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9200 - val_loss: 3.3347\n",
      "Epoch 44/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7856 - val_loss: 3.3594\n",
      "Epoch 45/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7981 - val_loss: 3.2988\n",
      "Epoch 46/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5817 - val_loss: 3.3492\n",
      "Epoch 47/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5536 - val_loss: 3.3549\n",
      "Epoch 48/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6102 - val_loss: 3.2765\n",
      "Epoch 49/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6507 - val_loss: 3.2549\n",
      "Epoch 50/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.6866 - val_loss: 3.4189\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Mean Squared Error: 3.747364755749752\n",
      "R^2 Score: 0.839632757458655\n"
     ]
    }
   ],
   "source": [
    "# Different activation function\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(64, input_dim=X_train.shape[1]))\n",
    "model4.add(LeakyReLU(alpha=0.1))\n",
    "model4.add(Dropout(0.2))\n",
    "model4.add(Dense(32))\n",
    "model4.add(LeakyReLU(alpha=0.1))\n",
    "model4.add(Dense(1))  # Output layer for regression\n",
    "\n",
    "# Compile the model\n",
    "model4.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "history = model4.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model4.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We incorporate batch normalization layers to improve training stability and speed._\n",
    "\n",
    "#### Model Architecture:\n",
    "\n",
    "**First Hidden Layer:**\n",
    "Dense layer with 128 neurons.\n",
    "BatchNormalization layer to normalize inputs.\n",
    "LeakyReLU activation function.\n",
    "Dropout layer with a rate of 0.3.\n",
    "\n",
    "**Second Hidden Layer:**\n",
    "Dense layer with 64 neurons.\n",
    "BatchNormalization layer.\n",
    "LeakyReLU activation function.\n",
    "\n",
    "**Output Layer:**\n",
    "Dense layer with 1 neuron for regression output.\n",
    "Compile, Train, and Evaluate:\n",
    "Follow the same procedure as before.\n",
    " \n",
    "##### Purpose:\n",
    "To see if batch normalization can enhance performance by reducing internal covariate shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 45.7853 - val_loss: 12.5251\n",
      "Epoch 2/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 6.5812 - val_loss: 4.5752\n",
      "Epoch 3/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 5.9663 - val_loss: 4.3941\n",
      "Epoch 4/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 5.8837 - val_loss: 4.0560\n",
      "Epoch 5/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5481 - val_loss: 4.0406\n",
      "Epoch 6/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.6727 - val_loss: 4.0076\n",
      "Epoch 7/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5474 - val_loss: 3.9125\n",
      "Epoch 8/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.0747 - val_loss: 4.0111\n",
      "Epoch 9/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0703 - val_loss: 3.7179\n",
      "Epoch 10/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.8531 - val_loss: 3.7027\n",
      "Epoch 11/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 5.0513 - val_loss: 3.8344\n",
      "Epoch 12/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 5.0255 - val_loss: 3.5558\n",
      "Epoch 13/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.7260 - val_loss: 3.8458\n",
      "Epoch 14/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.6934 - val_loss: 3.7323\n",
      "Epoch 15/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.8994 - val_loss: 3.3969\n",
      "Epoch 16/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.7620 - val_loss: 3.4577\n",
      "Epoch 17/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.5938 - val_loss: 3.3348\n",
      "Epoch 18/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.8322 - val_loss: 3.3995\n",
      "Epoch 19/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.3005 - val_loss: 3.2465\n",
      "Epoch 20/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5295 - val_loss: 3.3198\n",
      "Epoch 21/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.5031 - val_loss: 3.7334\n",
      "Epoch 22/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.2849 - val_loss: 3.2931\n",
      "Epoch 23/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3964 - val_loss: 3.3236\n",
      "Epoch 24/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.5339 - val_loss: 3.3478\n",
      "Epoch 25/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.4508 - val_loss: 3.2875\n",
      "Epoch 26/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.4805 - val_loss: 3.2888\n",
      "Epoch 27/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.4065 - val_loss: 3.2555\n",
      "Epoch 28/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2849 - val_loss: 3.2096\n",
      "Epoch 29/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3009 - val_loss: 3.2536\n",
      "Epoch 30/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3494 - val_loss: 3.3756\n",
      "Epoch 31/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0980 - val_loss: 3.2647\n",
      "Epoch 32/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3641 - val_loss: 3.2174\n",
      "Epoch 33/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3484 - val_loss: 3.1900\n",
      "Epoch 34/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2362 - val_loss: 3.2161\n",
      "Epoch 35/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0807 - val_loss: 3.1643\n",
      "Epoch 36/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0651 - val_loss: 3.1157\n",
      "Epoch 37/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.3144 - val_loss: 3.2768\n",
      "Epoch 38/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3.8692 - val_loss: 3.1796\n",
      "Epoch 39/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1165 - val_loss: 3.1931\n",
      "Epoch 40/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2386 - val_loss: 3.1885\n",
      "Epoch 41/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2237 - val_loss: 3.5193\n",
      "Epoch 42/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1786 - val_loss: 3.1835\n",
      "Epoch 43/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0456 - val_loss: 3.2473\n",
      "Epoch 44/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1973 - val_loss: 3.3188\n",
      "Epoch 45/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3.9918 - val_loss: 3.1461\n",
      "Epoch 46/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.1923 - val_loss: 3.1423\n",
      "Epoch 47/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9003 - val_loss: 3.2442\n",
      "Epoch 48/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.0972 - val_loss: 3.1612\n",
      "Epoch 49/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1695 - val_loss: 3.0650\n",
      "Epoch 50/50\n",
      "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.2586 - val_loss: 3.2138\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Mean Squared Error: 3.5310900126844276\n",
      "R^2 Score: 0.8488881639742624\n"
     ]
    }
   ],
   "source": [
    "#Add Batch Normalization\n",
    "\n",
    "model5 = Sequential()\n",
    "model5.add(Dense(128, input_dim=X_train.shape[1]))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(LeakyReLU(alpha=0.1))\n",
    "model5.add(Dropout(0.3))\n",
    "model5.add(Dense(64))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(LeakyReLU(alpha=0.1))\n",
    "model5.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model5.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "history = model5.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model5.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We shift from neural networks to tree-based ensemble methods by implementing an XGBoost regressor._\n",
    "\n",
    "#### Model Definition:\n",
    "**Use XGBRegressor with:**\n",
    "n_estimators=100: Number of gradient boosted trees.\n",
    "learning_rate=0.1: Step size shrinkage.\n",
    "max_depth=6: Maximum depth of a tree.\n",
    "random_state=42: For reproducibility.\n",
    "\n",
    "**Train the Model:**\n",
    "Fit the model on the training data.\n",
    "Predict and Evaluate:\n",
    "Make predictions on the test set.\n",
    "Evaluate using MSE and R² Score.\n",
    "\n",
    "**Purpose:**\n",
    "To compare the performance of advanced ensemble methods with neural networks on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.4488515357083847\n",
      "R^2 Score: 0.895202203742742\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion - XGBoost Outperforms Other Models\n",
    "After experimenting with various models, we conclude that the _XGBoost regressor_ delivers the best performance.\n",
    "\n",
    "**Performance Comparison:**\n",
    "_XGBoost_ achieved the lowest Mean Squared Error and the highest R² Score among all tested models.\n",
    "_Neural network models_, despite architectural changes, did not outperform _XGBoost_.\n",
    "\n",
    "**Final Result:**\n",
    "_XGBoost_ is the best model for predicting race positions in this dataset.\n",
    "\n",
    "**Implications:**\n",
    "Tree-based ensemble methods like _XGBoost_ can be more effective for tabular data with complex relationships.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
